#  Orquestaci贸n de Datos con Apache Airflow en Google Cloud Platform 锔

隆Bienvenido a mi repositorio! Aqu铆 encontrar谩s el c贸digo en Python que utilizo para orquestar un flujo de trabajo de transformaci贸n de datos con Apache Airflow y Google Cloud Platform (GCP). Este proyecto abarca varias etapas clave:

1. **Creaci贸n de un Cluster Dataproc**: Utilizo Airflow para programar la creaci贸n de un cl煤ster Dataproc en GCP, proporcionando un entorno escalable para ejecutar trabajos de procesamiento de datos.

2. **Ejecuci贸n de Trabajos en Dataproc**: Defino y env铆o trabajos de transformaci贸n de datos al cl煤ster Dataproc reci茅n creado. Estos trabajos aprovechan la potencia de procesamiento distribuido para realizar operaciones complejas de manera eficiente.

3. **Almacenamiento en Cloud Storage**: Despu茅s de la transformaci贸n, almaceno los resultados en Cloud Storage, aprovechando la capacidad de almacenamiento y la durabilidad de GCP.

4. **Carga de Datos en BigQuery**: Airflow tambi茅n orquesta la carga de datos desde Cloud Storage a BigQuery, permitiendo un an谩lisis escalable y eficiente de los datos transformados.

## Estructura del Proyecto

- `dag-etl.py`: Este es el archivo donde defino el DAG en Airflow.
- `jobs/`: Contiene los scripts de transformaci贸n de datos enviados a Dataproc.

## Configuraci贸n

Antes de ejecutar los DAGs, configura las credenciales de GCP y ajusta los par谩metros en los archivos de configuraci贸n.

隆Gracias por explorar este proyecto conmigo!
